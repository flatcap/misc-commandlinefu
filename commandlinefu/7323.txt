Remove duplicate rows of an un-sorted file based on a single column

perl -ane 'print unless $x{$F[0]}++' infile > outfile
F[0] filters using first word. $F[1] - 2nd, and so on.

Alternatives

Terminal - Alternatives
awk '{ if ($1 in stored_lines) x=1; else print; stored_lines[$1]=1 }'
infile.txt > outfile.txt

The command (above) will remove any duplicate rows based on the FIRST
column of data in an un-sorted file.

The '$1' represents a positional parameter. You can change both instances
of '$1' in the command to remove duplicates based on a different column,
for instance, the third:

awk '{ if ($3 in stored_lines) x=1; else print; stored_lines[$3]=1 }'
infile.txt > outfile.txt

Or you can change it to '$0' to base the removal on the whole row:

awk '{ if ($0 in stored_lines) x=1; else print; stored_lines[$0]=1 }'
infile.txt > outfile.txt

** Note: I wouldn't use this on a MASSIVE file, unless you're RAM-rich ;)
**
